---
title: "JSC370 Final Report"
author: "Yongzhen Huang"
date: "4/18/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning=F, message=F, result=F)
```

# Introduction
We currently live in a special time in history. While we are still dealing with an ongoing global pandemic,
the economy is also faced with high inflation. High inflation often leads to higher cost of living and decreased
living standard. 

This project aims to answer how inflation is tied to an economy and the economic growth, and how one may predict the future inflation given current state of the economy.
In particular, the following questions will be answered

1. How does inflation affect the overall economy and economic growth (e.g. GDP)?
2. How does inflation affect the standard of living i.e. CPI?
3. Given the current state of the economy, how may one predict the future inflation?


# Methods
## Data Source
The main source of data is from Organisation for Economic Co-operation and Development ([OECD](https://www.oecd.org/)).
Data from OECD are downloaded as annual data in CSV files and includes the following information.

| Category        |                 Details                  |
|----------------:|-----------------------------------------:|
| GDP             | $USD millions, per capita                |
| Population      | # million persons, growth, % working age |
| Employment Rate | % of working age population              |
| Inflation       | annual %                                 |
| Prices          | housing (rent) and share prices          |
| Interest Rate   | government short-term interest rate      |
| Household stats | Savings and spending                     |

Stock data are mainly extracted from [Alpha Vantage](https://www.alphavantage.co/) and [Finnhub](https://finnhub.io/).

The API for Alpha Vantage and Finnhub are only available in Python and other non-R languages, and therefore the data is extracted
in the code [here](https://hgeorge21.github.io/JSC370-finalproject/data_download.py)


## Data Cleaning
Overall, the main tools used for the project are R and Python.

Python is focused towards data collection from Finnhub and Alpha Vantage for
market, mainly major indices (DJIA, NASAQ, and S&P500) with sector ETFs.

R is used for the data cleaning, analytics, and constructing interactive plots

* `tidylr` for piping
* `dplyr` for table manipulations
* `data.tables` for more table manipulations
* `ggplot` and `plotly` for visualization

Once data is read in from CSV to dataframe (one file per metric), they are filtered
to contain only the information described above. Then they are all merged using `merge` based on
country code and year. From here, any NA are dropped.

For the prediction problem, the target is next year's inflation number for each country. This data
is first extracted from consolidated data to update the year and then merged back as target. Finally,
the train and test data are split in 70% and 30%, respectively, in chronological order i.e. the test
data occurs strictly after the training data by country.


# Results
```{r echo=F}
fpath <- "../data/"

fn <- "https://raw.githubusercontent.com/hgeorge21/JSC370-actualfinalproject/main/resource/datacleaning.R"
if (!file.exists("./datacleaning.R"))
  download.file(fn, destfile = "./datacleaning.R")
source('./datacleaning.R')
```

## Predictive Models
### Decision Tree
```{r echo=F, fig.cap="Pruned Decision Tree"}
set.seed(15129)

# Decision Tree
tree <- rpart(Target~., data=train, method="anova", control=list(minsplit=10, minbucket=3, cp=0, xval=10))
optimalcp = tree$cptable[which.min(tree$cptable[,"xerror"]), "CP"]
tree_prune <- prune(tree, cp=optimalcp)
rpart.plot(tree_prune)
```

### Bagging
```{r echo=F, fig.cap="Bagging Variable Importance"}
set.seed(15129)

# Bagging
bag <- randomForest(Target~., data=train, mtry=11, na.action=na.omit)
varImpPlot(bag, n.var=11, col="blue", main='Bagging Importance Plot')
```

### Random Forest
```{r echo=F, fig.cap="Random Forest Variable Importance"}
set.seed(15129)

# random forest
rf <- randomForest(Target~., data=train, na.action=na.omit)
varImpPlot(rf, n.var=11, col="blue", main='Random Forest Importance Plot')
```

### GBM
```{r echo=F, fig.cap="GBM Shrinkage (left), Variable Importance (right)"}
set.seed(15129)

# GBM
shrinkages <- seq(0.001, 0.1, length.out=10)
mses <- c()
for (skg in shrinkages) {
  boost = gbm(Target~., data=train, distribution='gaussian', n.trees=1000, shrinkage=skg, interaction.depth=1, cv.folds=10)
  mses <- c(mses, mean(boost$train.error))
}

tmp <- data.frame(shrinkage=shrinkages,mse=mses)
ggplot(tmp, aes(y=mse, x=shrinkage)) +
  geom_point() +
  xlab('Shrinkage') +
  ylab('MSE')

set.seed(15129)

boost <- gbm(Target~., data=train, distribution='gaussian', n.trees=1000, shrinkage=0.1, interaction.depth=1, cv.folds=10)
summary <- summary(boost)

knitr::kable(data.frame(Feature=summary$var,
                        Relative.Influence=summary$rel.inf),
             caption="Variable Importance")
```

### XGBoost
```{r echo=F, fig.cap="XGBoost Variable Importance"}
set.seed(15129)

# XGBoost
train_control = trainControl(method="cv", number=10, search="grid")

tune_grid <- expand.grid(max_depth = 3,
                         nrounds = (1:10)*50,
                         eta = seq(0.001, 0.5, length.out=10),
                         gamma = 0,
                         subsample = 1, 
                         min_child_weight = 1, 
                         colsample_bytree = 0.6
                         )

xgb <- caret::train(Target~., data=train, method="xgbTree", trControl = train_control, tuneGrid = tune_grid, verbosity=0)
plot(varImp(xgb, scale = F))
```

## Model Performance
```{r echo=F}
methods <- c('Regression Tree', 'Bagging', 'Random Forest', 'Boosting', 'XGBoost')
rmses <- c()

yhat_tree <- predict(tree_prune, newdata=test)
rmses <- c(rmses, mean((yhat_tree - test[,"Target"])^2))

yhat_bag <- predict(bag, newdata=test)
rmses <- c(rmses, mean((yhat_bag - test[,"Target"])^2))

yhat_rf <- predict(rf, newdata=test)
rmses <- c(rmses, mean((yhat_rf - test[,"Target"])^2))

yhat_boost <- predict(boost, newdata=test, n.trees=1000)
rmses <- c(rmses, mean((yhat_boost - test[,"Target"])^2))

yhat_xgb <- predict(xgb, newdata = test)
rmses <- c(rmses, caret::RMSE(test[, "Target"], yhat_xgb)^2)

knitr::kable(data.frame(method=methods, RMSE=round(rmses,4)),
             caption="Methods and RMSE")
```


The following graph shows the performance of the XGBoost by plotting the predicted
inflation rate against the actual inflation rate. The red line is the 45-degree line
which indicates the case of perfect prediction where all points should lie on the line.
From the graph, we can see there is a linear relationship between the predicted and 
actual inflation. The implication is that the XGBoost model is capable to predict
future inflation to some extent. This is important because it can help policy makers
to better understand the future state of economy and adjust policies accordingly.

```{r echo=F}
test['Prediction'] <- yhat_xgb
ggplot(test, aes(y=Target, x=Prediction)) +
  geom_point() +
  geom_abline(intercept=0, slope=1, col='red') +
  labs(title='XGBoost Predicted vs Actual Inflation Rate')+
  xlab('Predicted (%)') +
  ylab('Actual (%)')
```


# Conclusions and Summary






